---
title: Introduction
---


Unsupervised learning in high-dimensional data poses significant challenges, particularly in discovering interpretable features and enabling controllable generation.

Variational Autoencoders (VAEs) provide a probabilistic framework for mapping complex data to lower-dimensional latent spaces, but they often fail to disentangle underlying factors of variation, especially when the number of true sources is unknown or when observations exhibit diverse attribute combinations. 

The ["Variational Sparse Coding"](https://proceedings.mlr.press/v115/tonolini20a.html) (VSC) paper by Francesco Tonolini et al. proposes a novel extension of VAEs, incorporating sparsity in the latent space via a Spike and Slab prior to address these issues.

This report for a statistical course explores the VSC model, detailing its theoretical foundations, implementation, and empirical validation.

## Datasets

* **MNIST** : TODO...
* **Fashion-MNIST**: A collection of 28x28 grayscale images of clothing items (e.g., T-shirts, trousers), comprising 60,000 training and 10,000 test samples. It serves as a drop-in replacement for MNIST, offering richer variability for testing generative models.
* **Smiley** : TODO...
* **UCI HAR**: A dataset of accelerometer and gyroscope signals from smartphones, capturing six human activities (e.g., walking, sitting) across 30 subjects, with preprocessed segments of 128 time steps.

## Report Structure

* **Autoencoders**: Introduces the basics of autoencoders and their role in high-dimensional data analysis.
* **Variational Autoencoders**: Explains VAEs, focusing on the Evidence Lower Bound (ELBO) and latent space regularization.
* **Variational Sparse Coding**: Details the VSC model, including the Spike and Slab prior and warm-up training strategy.
* **Experiments**: Summarizes empirical results, comparing VSC to β-VAEs across multiple tasks.
* **Conclusion**: Highlights VSC’s contributions and future directions.